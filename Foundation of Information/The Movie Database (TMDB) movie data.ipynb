{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab26c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_html_to_file1(html_content, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(html_content))\n",
    "\n",
    "def save_html_to_file(html_content, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67d866",
   "metadata": {},
   "source": [
    "###### 1. Establish a connection to the webpage - \"https://www.themoviedb.org/movie\" - and provide the following details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1. Import the requests library\n",
    "import requests\n",
    "\n",
    "# a2. Formulate a get request to download the contents of the webpage\n",
    "needed_headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 \\\n",
    "(KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36\"}\n",
    "\n",
    "response = requests.get((\"https://www.themoviedb.org/movie\"),headers = needed_headers)\n",
    "\n",
    "# b. Verify the status code of the request and confirm that the request was executed appropriately\n",
    "if response.status_code == 200:\n",
    "    print(\"Request executed successfully.\")\n",
    "    \n",
    "    # c. Print the contents of the page obtained from the response and save it in a variable\n",
    "    page_content = response.content\n",
    "    print(page_content.decode('utf-8'))\n",
    "    save_html_to_file(page_content, \"Desktop/page_content.html\")\n",
    "\n",
    "    # d. Infer the type of the variable created in part 1c and display the first 200 characters of the content from the server’s response\n",
    "    print(f\"Type of page_content: {type(page_content)}\")\n",
    "    print(f\"First 200 characters of the content:\\n{page_content[:200].decode('utf-8')}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8ef48",
   "metadata": {},
   "source": [
    "###### 2. Parse the content of HTML response using the BeautifulSoup library and execute the tasks specified in the guidelines mentioned below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1. From the BeautifulSoup library (bs4) import the BeautifulSoup class\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# a2. Pass the contents of the webpage obtained from step 1c as an argument to create an instance of the BeautifulSoup class\n",
    "html_content = page_content.decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# b. Extract the title of the parsed web page content using an appropriate method or attribute of the document object created in part 2a\n",
    "title = soup.title.string\n",
    "print(f\"Title of the webpage: {title}\")\n",
    "\n",
    "# c. Write a user defined function to generalize the task presented in Q2a to any URL that retrieves the content of the webpage. Your function should take a URL string as an input and return a correctly formulated BeautifulSoup instance as the output. In your function definition, ensure that appropriate exceptions are raised to the user (through status codes) if they pass in malformed/incorrect URLs. Write two test cases for your function - one with a working URL and another with an URL that gets a 404 response.\n",
    "def create_beautifulsoup_instance(url):\n",
    "    try:\n",
    "        # Make a request to the URL\n",
    "        response = requests.get(url, headers = needed_headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"HTTP Error: {err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Request Error: {err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An unexpected error occurred: {err}\")\n",
    "\n",
    "# Test case with a working URL\n",
    "url_working = \"https://www.themoviedb.org/movie\"\n",
    "soup_working = create_beautifulsoup_instance(url_working)\n",
    "print(soup_working.title.string)\n",
    "\n",
    "# Test case with a URL that gets a 404 response\n",
    "url_not_found = \"https://www.themoviedb.org/movies\"\n",
    "soup_not_found = create_beautifulsoup_instance(url_not_found)\n",
    "# The function will print the error message for a 404 response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3c8b1",
   "metadata": {},
   "source": [
    "###### 3. Extract the content of the webpage - https://www.themoviedb.org/movie - that hosts a current dated listing of popular movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.Write a function call to the user defined function created in 2c with the url https://www.themoviedb.org/movie as an input and store the response in a variable\n",
    "url_popular_movies = \"https://www.themoviedb.org/movie\"\n",
    "\n",
    "soup_popular_movies = create_beautifulsoup_instance(url_popular_movies)\n",
    "\n",
    "# b.Print the HTML content associated with the first movie displayed on the web page using appropriate HTML tags to access this listing on the object created in part 3a \n",
    "first_movie_html = soup_popular_movies.find('div', class_='page_wrap movie_wrap')\n",
    "print(first_movie_html)\n",
    "filename = \"Desktop/first_movie_content.html\"\n",
    "save_html_to_file1(first_movie_html, filename)\n",
    "\n",
    "# c.Display the name of the first movie using appropriate HTML tags to access this listing on the object created in part 3a\n",
    "movie_name_found = False\n",
    "if first_movie_html:\n",
    "    column_movies = first_movie_html.find('div', class_=\"white_column no_pad\")\n",
    "    if column_movies:\n",
    "        movie_content = column_movies.find('div', class_= \"content\")\n",
    "        if movie_content:\n",
    "            h2_tag = movie_content.find(\"h2\")\n",
    "            if h2_tag:\n",
    "                movie_name_found = True\n",
    "                print(f\"Name of the first movie: {h2_tag.a.text}\")\n",
    "\n",
    "if movie_name_found == False:\n",
    "    print(\"Unable to find the name of the first movie.\")\n",
    "\n",
    "    \n",
    "# d.Display the user rating of the first movie by using appropriate HTML tags to access this listing on the object created in part 3a\n",
    "rating_found = False\n",
    "if movie_content:\n",
    "    consensus_tag = movie_content.find('div', class_=\"consensus tight\")\n",
    "    if consensus_tag:\n",
    "        outer_ring_tag = consensus_tag.find('div', class_=\"outer_ring\")\n",
    "        if outer_ring_tag:\n",
    "            user_score_tag = outer_ring_tag.find('div', class_=\"user_score_chart 613a9325e272600063455943\")\n",
    "            if user_score_tag:\n",
    "                rating_found = True\n",
    "                rating = user_score_tag[\"data-percent\"]\n",
    "                print(f\"Rating of the first movie: {rating} %\")\n",
    "\n",
    "if rating_found == False:\n",
    "    print(\"Unable to find the rating of the first movie.\")  \n",
    "\n",
    "# e.For the first movie, extract the part of the url following the string “https://www.themoviedb.org/” using the appropriate HTML tags to extract this portion on the object created in part 3a\n",
    "part_of_url = False\n",
    "if first_movie_html:\n",
    "    column_movies = first_movie_html.find('div', class_=\"white_column no_pad\")\n",
    "    if column_movies:\n",
    "        movie_content = column_movies.find('div', class_= \"content\")\n",
    "        if movie_content:\n",
    "            h2_tag = movie_content.find(\"h2\")\n",
    "            if h2_tag:\n",
    "                part_of_url = True\n",
    "                part = h2_tag.a['href']\n",
    "                print(f\"Part of the URL: {part}\")\n",
    "\n",
    "if part_of_url == False:\n",
    "    print(\"Unable to find the part of the url for the first movie.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd539dd",
   "metadata": {},
   "source": [
    "###### 4. Write user defined functions for each subsection below (i.e., Q4 a, Q4b, Q4c, Q4d, and Q4e) to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee99d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# a.Titles of all the movies on the page as a Python list\n",
    "def get_movie_titles(soup):\n",
    "    movie_titles = []\n",
    "    first_movie_html = soup.find('div', class_='page_wrap movie_wrap')\n",
    "\n",
    "    if first_movie_html:\n",
    "        column_movies = first_movie_html.find('div', class_=\"white_column no_pad\")\n",
    "        if column_movies:\n",
    "            for movie_content in column_movies.find_all('div', class_= \"content\"):\n",
    "                if movie_content:\n",
    "                    h2_tag = movie_content.find(\"h2\")\n",
    "                    if h2_tag:\n",
    "                        movie_titles.append(h2_tag.a.text)\n",
    "\n",
    "    return movie_titles\n",
    "\n",
    "# b. User ratings of all the movies on the page as a Python list\n",
    "def get_user_ratings(soup):\n",
    "    user_ratings = []\n",
    "    first_movie_html = soup.find('div', class_='page_wrap movie_wrap')\n",
    "\n",
    "    if first_movie_html:\n",
    "        column_movies = first_movie_html.find('div', class_=\"white_column no_pad\")\n",
    "        if column_movies:\n",
    "            for movie_content in column_movies.find_all('div', class_= \"content\"):\n",
    "                if movie_content:\n",
    "                    user_score_tag = movie_content.find('div', class_=re.compile(r'^user_score_chart \\d+'))\n",
    "                    if user_score_tag:\n",
    "                        user_ratings.append(user_score_tag[\"data-percent\"])\n",
    "                    else:\n",
    "                        user_ratings.append(\"not rated\")\n",
    "    return user_ratings\n",
    "\n",
    "# c. HTML content of all the individual pages of movies collected into a Python list\n",
    "def get_html_contents(soup):\n",
    "    html_contents = []\n",
    "    first_movie_html = soup.find('div', class_='page_wrap movie_wrap')\n",
    "\n",
    "    if first_movie_html:\n",
    "        column_movies = first_movie_html.find('div', class_=\"white_column no_pad\")\n",
    "        if column_movies:\n",
    "            for movie_content in column_movies.find_all('div', class_= \"content\"):\n",
    "                if movie_content:\n",
    "                    h2_tag = movie_content.find(\"h2\")\n",
    "                    if h2_tag:\n",
    "                        part = h2_tag.a['href']\n",
    "                        html_contents.append(part)\n",
    "\n",
    "    return html_contents\n",
    "\n",
    "# d. Genres of all the movies on the page as a Python list\n",
    "def get_movie_genres(movie_html_content_list):\n",
    "    genres_list = []\n",
    "    for html_content in movie_html_content_list:\n",
    "        soup = create_beautifulsoup_instance(\"https://www.themoviedb.org\" + html_content)\n",
    "        genres = [genre.text.strip().replace('\\xa0', '') for genre in soup.find_all('span', class_='genres')]\n",
    "        genres_list.append(genres)\n",
    "    return genres_list\n",
    "\n",
    "# e. Cast of all the movies on the page as a Python list\n",
    "def get_movie_cast(movie_html_content_list):\n",
    "    cast_list = []\n",
    "    for html_content in movie_html_content_list:\n",
    "        soup = create_beautifulsoup_instance(\"https://www.themoviedb.org\" + html_content)\n",
    "        cast_scroller = soup.find('div', class_='scroller_wrap should_fade is_fading')\n",
    "        if cast_scroller:\n",
    "            ordered_list = soup.find('ol', class_='people scroller')\n",
    "            list_items = ordered_list.find_all('li')\n",
    "            curr_list = []\n",
    "            for item in list_items:\n",
    "                img_tag = item.find('img')\n",
    "                if img_tag:\n",
    "                    cast = img_tag['alt']\n",
    "                    curr_list.append(cast)\n",
    "            cast_list.append(curr_list)\n",
    "    return cast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee13e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_movie_titles(soup_popular_movies)\n",
    "print(titles)\n",
    "\n",
    "ratings = get_user_ratings(soup_popular_movies)\n",
    "print(ratings)\n",
    "\n",
    "contents = get_html_contents(soup_popular_movies)\n",
    "print(contents)\n",
    "genres = get_movie_genres(contents)\n",
    "print(genres)\n",
    "casts = get_movie_cast(contents)\n",
    "print(casts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c261d3",
   "metadata": {},
   "source": [
    "###### 5. Write an user defined function that returns a pandas data frame with following data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c12293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_movie_dataframe(soup, contents):\n",
    "    movie_data = []\n",
    "\n",
    "    # a. Titles of the movies listed on the page\n",
    "    movie_titles = get_movie_titles(soup)\n",
    "    movie_data.append(movie_titles)\n",
    "    \n",
    "    # b. User ratings of the movies listed on the page\n",
    "    user_ratings = get_user_ratings(soup)\n",
    "    movie_data.append(user_ratings)\n",
    "    \n",
    "    # c. Genres of the movies listed on the page\n",
    "    movie_genres = get_movie_genres(contents)\n",
    "    movie_data.append(movie_genres)\n",
    "    \n",
    "    # d. Cast of the movies listed on the page\n",
    "    movie_cast = get_movie_cast(contents)\n",
    "    movie_data.append(movie_cast)\n",
    "\n",
    "    df = pd.DataFrame(movie_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4978bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_movie_dataframe(soup_popular_movies, contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806192a6",
   "metadata": {},
   "source": [
    "###### 6. Scraping the data and combining the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.Write a function that scrapes data (mentioned in Q5) from page number 1, 2, 3, 4 and 5 on the URL https://www.themoviedb.org/movie and returns 5 data frames which can be exported to csv file by calling the functions defined in Q3a, Q4c and Q5 \n",
    "def scrape_movies_from_pages(pages):\n",
    "    all_dataframes = []\n",
    "    for page in pages:\n",
    "        url = f\"https://www.themoviedb.org/movie?page={page}\"\n",
    "        soup = create_beautifulsoup_instance(url)\n",
    "        contents = get_html_contents(soup)\n",
    "        \n",
    "        df = create_movie_dataframe(soup, contents)\n",
    "        df.to_csv(f'Desktop/page_{page}_data.csv', index=False)\n",
    "        all_dataframes.append(df)\n",
    "    return all_dataframes\n",
    "\n",
    "# b. Combine the data obtained from dataframes in Q6(a)\n",
    "def combine_dataframes(dataframes):\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bf1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1, 2, 3, 4, 5]\n",
    "all_dfs = scrape_movies_from_pages(pages)\n",
    "df = combine_dataframes(all_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
